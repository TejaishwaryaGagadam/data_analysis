{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Exploring Fairness When Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(6)\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import GermanDataset, StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data\n",
    "\n",
    "The German Credit Risk dataset contains 1000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes. The original dataset can be found at https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read in the aif360 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = GermanDataset(protected_attribute_names=['age'],           \n",
    "                             privileged_classes=[lambda x: x >= 25], \n",
    "                             features_to_drop=['personal_status', 'sex'])      # age >=25 is considered privileged\n",
    "\n",
    "# Store definitions of priviledged and unpriviledged groups\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split into train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (600, 58)\n",
      "Val set:  (200, 58)\n",
      "Test set:  (200, 58)\n"
     ]
    }
   ],
   "source": [
    "# Split original data into train and test data\n",
    "train_orig, test_orig = dataset_orig.split([0.8], shuffle=True, seed=10)\n",
    "# Split training data in to training and validation data for hyperparameter tuning\n",
    "train_orig, val_orig = train_orig.split([0.75], shuffle=True)\n",
    "\n",
    "# Convert to dataframes\n",
    "train_orig_df, _ = train_orig.convert_to_dataframe()\n",
    "val_orig_df, _ = val_orig.convert_to_dataframe()\n",
    "test_orig_df, _ = test_orig.convert_to_dataframe()\n",
    "\n",
    "print(\"Train set: \", train_orig_df.shape)\n",
    "print(\"Val set: \", val_orig_df.shape)\n",
    "print(\"Test set: \", test_orig_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['month', 'credit_amount', 'investment_as_income_percentage',\n",
      "       'residence_since', 'age', 'number_of_credits', 'people_liable_for',\n",
      "       'status=A11', 'status=A12', 'status=A13', 'status=A14',\n",
      "       'credit_history=A30', 'credit_history=A31', 'credit_history=A32',\n",
      "       'credit_history=A33', 'credit_history=A34', 'purpose=A40',\n",
      "       'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43',\n",
      "       'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48',\n",
      "       'purpose=A49', 'savings=A61', 'savings=A62', 'savings=A63',\n",
      "       'savings=A64', 'savings=A65', 'employment=A71', 'employment=A72',\n",
      "       'employment=A73', 'employment=A74', 'employment=A75',\n",
      "       'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103',\n",
      "       'property=A121', 'property=A122', 'property=A123', 'property=A124',\n",
      "       'installment_plans=A141', 'installment_plans=A142',\n",
      "       'installment_plans=A143', 'housing=A151', 'housing=A152',\n",
      "       'housing=A153', 'skill_level=A171', 'skill_level=A172',\n",
      "       'skill_level=A173', 'skill_level=A174', 'telephone=A191',\n",
      "       'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202',\n",
      "       'credit'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>investment_as_income_percentage</th>\n",
       "      <th>residence_since</th>\n",
       "      <th>age</th>\n",
       "      <th>number_of_credits</th>\n",
       "      <th>people_liable_for</th>\n",
       "      <th>status=A11</th>\n",
       "      <th>status=A12</th>\n",
       "      <th>status=A13</th>\n",
       "      <th>...</th>\n",
       "      <th>housing=A153</th>\n",
       "      <th>skill_level=A171</th>\n",
       "      <th>skill_level=A172</th>\n",
       "      <th>skill_level=A173</th>\n",
       "      <th>skill_level=A174</th>\n",
       "      <th>telephone=A191</th>\n",
       "      <th>telephone=A192</th>\n",
       "      <th>foreign_worker=A201</th>\n",
       "      <th>foreign_worker=A202</th>\n",
       "      <th>credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2712.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2503.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>36.0</td>\n",
       "      <td>3535.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6361.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month  credit_amount  investment_as_income_percentage  residence_since  \\\n",
       "822   36.0         2712.0                              2.0              2.0   \n",
       "830   24.0         2375.0                              4.0              2.0   \n",
       "703   30.0         2503.0                              4.0              2.0   \n",
       "776   36.0         3535.0                              4.0              4.0   \n",
       "659   18.0         6361.0                              2.0              1.0   \n",
       "\n",
       "     age  number_of_credits  people_liable_for  status=A11  status=A12  \\\n",
       "822  1.0                1.0                2.0         1.0         0.0   \n",
       "830  1.0                2.0                2.0         0.0         0.0   \n",
       "703  1.0                2.0                1.0         0.0         1.0   \n",
       "776  1.0                2.0                1.0         0.0         0.0   \n",
       "659  1.0                1.0                1.0         0.0         1.0   \n",
       "\n",
       "     status=A13  ...  housing=A153  skill_level=A171  skill_level=A172  \\\n",
       "822         0.0  ...           0.0               0.0               0.0   \n",
       "830         0.0  ...           0.0               0.0               0.0   \n",
       "703         0.0  ...           0.0               0.0               0.0   \n",
       "776         0.0  ...           0.0               0.0               0.0   \n",
       "659         0.0  ...           0.0               0.0               0.0   \n",
       "\n",
       "     skill_level=A173  skill_level=A174  telephone=A191  telephone=A192  \\\n",
       "822               1.0               0.0             1.0             0.0   \n",
       "830               1.0               0.0             0.0             1.0   \n",
       "703               1.0               0.0             1.0             0.0   \n",
       "776               1.0               0.0             0.0             1.0   \n",
       "659               1.0               0.0             0.0             1.0   \n",
       "\n",
       "     foreign_worker=A201  foreign_worker=A202  credit  \n",
       "822                  1.0                  0.0     2.0  \n",
       "830                  1.0                  0.0     1.0  \n",
       "703                  1.0                  0.0     1.0  \n",
       "776                  1.0                  0.0     1.0  \n",
       "659                  1.0                  0.0     1.0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_orig_df.columns)\n",
    "train_orig_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder of what we did last week, let's calculate some fairness metrics on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference = -0.135481\n",
      "Disparate Impact = 0.814721\n"
     ]
    }
   ],
   "source": [
    "train_orig_metrics = BinaryLabelDatasetMetric(train_orig, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Mean difference = %f\" % train_orig_metrics.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % train_orig_metrics.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a classifier to predict credit using the original data\n",
    "\n",
    "We will be training a logistic regression model to predict good/bad credit, then fine-tuning the model over a set of hyperparameters. Then, we'll see how well this basic model does on some fairness metrics. \n",
    "\n",
    "### 2.1 Training and evaluationg a logistic regression model \n",
    "First, we need to split our data up into the explantory variables (x) and the outcome variable (y). We will recode the outcome so that the values are 0 (= bad credit) and 1 (= good credit). This is the format that the sklearn logistic regression function expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcomes: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    426\n",
       "0.0    174\n",
       "Name: credit, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_orig_df.drop(\"credit\", axis=1)\n",
    "y_train = train_orig_df.credit.replace({2:0}) \n",
    "print(\"Outcomes: \")\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logistic regression model with the given hyperparameters\n",
    "initial_lr = LogisticRegression(C=0.5, penalty=\"l1\", solver='liblinear')\n",
    "    \n",
    "# Fit the model using the training data\n",
    "initial_lr = initial_lr.fit(x_train, y_train, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we should evaluate it on our validation set. For now, we'll look the AUC as well as accuracy when we use a cutoff of 0.5 (that is, predicted values over 0.5 are interpreted as good credit, and vice versa).\n",
    "\n",
    "Let's write a funciton to do that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y_true):\n",
    "    '''Calculates the AUC and accuracy for a trained logistic regression model'''\n",
    "    \n",
    "    # Calculate predicted values\n",
    "    y_pred = model.predict_proba(X)\n",
    "    # This returns a tuple for each observation containing the probability of being in each class.\n",
    "    # Since we're doing binary classification, all we need to know is the probability that the outcome = 1 (good credit)\n",
    "    y_pred = [row[1] for row in y_pred] # This pulls the predicted probability that y = 1 for each observation\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, [pred_prob >= 0.5 for pred_prob in y_pred])\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, auc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we call the function, we need to set up the validation data properly, the way we did for the training data.\n",
    "x_val = val_orig_df.drop(\"credit\", axis=1)\n",
    "y_val = val_orig_df.credit.replace({2:0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.73\n",
      "AUC:  0.7853520462318669\n"
     ]
    }
   ],
   "source": [
    "accuracy, auc = evaluate(initial_lr, x_val, y_val)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter tuning the logistic regression model \n",
    "\n",
    "For hyperparameter tuning, we want to be able to easily train models with a variety of hyperparameter and determine which one performs the best on the validation data. We can use the functions we wrote above to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_logistic_regression(train_df, val_df, penalty_types, C_values, weights=None, verbose=True):\n",
    "    '''Tunes logistic regression models over the hyperparameters penalty type and C\n",
    "       to maximize the AUC'''\n",
    "    # Pre-process the training and validation data\n",
    "    x_train = train_df.drop(\"credit\", axis=1)\n",
    "    y_train = train_df.credit.replace({2:0}) \n",
    "    x_val = val_df.drop(\"credit\", axis=1)\n",
    "    y_val = val_df.credit.replace({2:0}) \n",
    "\n",
    "    # Create empty lists where we will store the results of hyperparameter tuning \n",
    "    parameters = []\n",
    "    models = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    # Loop through the hyperparameters of interest\n",
    "    for penalty in penalty_types:\n",
    "        for C in C_values:\n",
    "            \n",
    "            # Train the logistic regression model with the given hyperparameters\n",
    "            lr = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
    "    \n",
    "            # Fit the model using the training data\n",
    "            lr = lr.fit(x_train, y_train, sample_weight=weights)\n",
    "            \n",
    "            # Get the evalution metrics on the validation set \n",
    "            accuracy, auc  = evaluate(lr, x_val, y_val)\n",
    "            \n",
    "            # Store the results\n",
    "            parameters.append({'penalty': penalty, 'C': C})\n",
    "            models.append(lr)\n",
    "            val_aucs.append(auc)\n",
    "            \n",
    "            # Print the results\n",
    "            if verbose:\n",
    "                print(\"\\nParmeters: \\tpenalty={} \\tC={}\".format(penalty, C))\n",
    "                print(\"Validtion AUC: {}\".format(auc))\n",
    "            \n",
    "    \n",
    "    # Determine the best model -- that is, the one with the AUC\n",
    "    best_model_index = np.argmax(val_aucs)\n",
    "    best_model = models[best_model_index]\n",
    "    \n",
    "    print(\"\\nBest model parameters: \", parameters[best_model_index])\n",
    "    print(\"Best model AUC: \", val_aucs[best_model_index])\n",
    "    \n",
    "    # Return best model\n",
    "    return best_model, parameters, models, val_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parmeters: \tpenalty=l1 \tC=0.001\n",
      "Validtion AUC: 0.4036442976766128\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=0.1\n",
      "Validtion AUC: 0.7456067932539214\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=1\n",
      "Validtion AUC: 0.7699021110980069\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=10\n",
      "Validtion AUC: 0.7450171010732397\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=100\n",
      "Validtion AUC: 0.7410071942446043\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=1000\n",
      "Validtion AUC: 0.741243071116877\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=10000\n",
      "Validtion AUC: 0.7413610095530133\n",
      "\n",
      "Parmeters: \tpenalty=l1 \tC=100000\n",
      "Validtion AUC: 0.7411251326807408\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=0.001\n",
      "Validtion AUC: 0.6545583205566695\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=0.1\n",
      "Validtion AUC: 0.7965561976648191\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=1\n",
      "Validtion AUC: 0.7690765420450525\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=10\n",
      "Validtion AUC: 0.7648307583441443\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=100\n",
      "Validtion AUC: 0.7653025120886896\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=1000\n",
      "Validtion AUC: 0.7668357117584621\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=10000\n",
      "Validtion AUC: 0.7648307583441444\n",
      "\n",
      "Parmeters: \tpenalty=l2 \tC=100000\n",
      "Validtion AUC: 0.7676612808114164\n",
      "\n",
      "Best model parameters:  {'penalty': 'l2', 'C': 0.1}\n",
      "Best model AUC:  0.7965561976648191\n"
     ]
    }
   ],
   "source": [
    "best_lr, parameters, models, val_aucs = tune_logistic_regression(train_orig_df, val_orig_df, penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results so that we understand what hyperparameter tuning actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_aucs_l1 = [val_aucs[i] for i in range(len(val_aucs)) if parameters[i]['penalty']==\"l1\"]\n",
    "val_aucs_l2 = [val_aucs[i] for i in range(len(val_aucs)) if parameters[i]['penalty']==\"l2\"]\n",
    "C_values = [parameters[i]['C'] for i in range(len(parameters)) if parameters[i]['penalty']==\"l2\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx( C_values, val_aucs_l1, marker='.', markerfacecolor='blue', markersize=12, color='blue', linewidth=4, label='L1 Penalty')\n",
    "ax.semilogx( C_values, val_aucs_l2, marker='.', markerfacecolor='red', markersize=12, color='red', linewidth=4, label='L2 Penalty')\n",
    "ax.set_xlabel(\"C\")\n",
    "ax.set_ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluating bias in our predictions\n",
    "\n",
    "Let's put our data back into a aif360 dataset format, so that we can use all of the fairness metrics provided by the package. For now, we'll evaluate bias on the training data. This mimics the development process we'd use in any real application.\n",
    "\n",
    "First, we'll get predicted values using the best model and attach them as a new column in the data frame. We'll use 0.5 as the threshold as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset\n",
    "train_preds_df = train_orig_df.copy()\n",
    "# Calculate predicted values\n",
    "train_preds_df['credit'] = best_lr.predict(x_train)\n",
    "# Recode the predictions so that they match the format that the dataset was originally provided in \n",
    "# (1 = good credit, 2 = bad credit)\n",
    "train_preds_df['credit'] = train_preds_df.credit.replace({0:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll create an object of the aif360 StandardDataset class. You can read more about this in the documentation:\n",
    "https://aif360.readthedocs.io/en/latest/modules/standard_datasets.html#aif360.datasets.StandardDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_aif360 = StandardDataset(train_orig_df, label_name='credit', protected_attribute_names=['age'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "preds_aif360 = StandardDataset(train_preds_df, label_name='credit', protected_attribute_names=['age'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate some fairness metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference = -0.221386\n",
      "Disparate Impact = 0.742480\n"
     ]
    }
   ],
   "source": [
    "pred_metrics = BinaryLabelDatasetMetric(preds_aif360, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Mean difference = %f\" % pred_metrics.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % pred_metrics.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from last week that we identified bias in the training data. We should therefore not find it surprising that we have bias in a model trained on that data.\n",
    "\n",
    "Now, since we have true values and predicted values, let's compare the true positive rate and false positive rate by group. This is similar to the analysis ProPublica did. \n",
    "\n",
    "Note that aif360 is pretty picky about what goes into this ClassificationMetric class, which is the reason for all the inefficient copying of datasets above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error rate difference (unprivileged error rate - privileged error rate)= 0.090362\n",
      "\n",
      "False negative rate for privledged groups = 0.054054\n",
      "False negative rate for unprivledged groups = 0.214286\n",
      "False negative rate ratio = 3.964286\n",
      "\n",
      "False positive rate for privledged groups = 0.625000\n",
      "False positive rate for unprivledged groups = 0.421053\n",
      "False positive rate ratio = 0.673684\n"
     ]
    }
   ],
   "source": [
    "orig_vs_preds_metrics = ClassificationMetric(orig_aif360, preds_aif360,\n",
    "                                                   unprivileged_groups=unprivileged_groups,\n",
    "                                                   privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_preds_metrics.error_rate_difference())\n",
    "\n",
    "\n",
    "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_preds_metrics.false_negative_rate(privileged=True))\n",
    "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_preds_metrics.false_negative_rate(privileged=False))\n",
    "print(\"False negative rate ratio = %f\" % orig_vs_preds_metrics.false_negative_rate_ratio())\n",
    "\n",
    "\n",
    "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_preds_metrics.false_positive_rate(privileged=True))\n",
    "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_preds_metrics.false_positive_rate(privileged=False))\n",
    "print(\"False positive rate ratio = %f\" % orig_vs_preds_metrics.false_positive_rate_ratio())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms it: our model is even *more* biased than the original credit scores.  \n",
    "\n",
    "Let's try to fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a classifier to predict credit using the original data, excluding the sensitive feature\n",
    "\n",
    "We've talked several times in class about how removing a sensitive attribute is not enough. Let's see if that's true in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model parameters:  {'penalty': 'l2', 'C': 0.1}\n",
      "Best model AUC:  0.7816959547116406\n"
     ]
    }
   ],
   "source": [
    "best_lr_noage, _, _, _ = tune_logistic_regression(train_orig_df.drop('age', axis=1), val_orig_df.drop('age', axis=1),\n",
    "                                                                       penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000], verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the AUC of our best model is *slightly* worse than before: by excluding a feature, we've lost some predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the same bias metrics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df_noage = train_orig_df.copy()\n",
    "preds_df_noage['credit'] = best_lr_noage.predict(x_train.drop('age', axis=1))\n",
    "preds_df_noage['credit'] = preds_df_noage.credit.replace({0:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference = -0.156337\n",
      "Disparate Impact = 0.815603\n",
      "\n",
      "Error rate difference (unprivileged error rate - privileged error rate)= 0.079724\n",
      "\n",
      "False negative rate for privledged groups = 0.062162\n",
      "False negative rate for unprivledged groups = 0.160714\n",
      "False negative rate ratio = 2.585404\n",
      "\n",
      "False positive rate for privledged groups = 0.602941\n",
      "False positive rate for unprivledged groups = 0.473684\n",
      "False positive rate ratio = 0.785623\n"
     ]
    }
   ],
   "source": [
    "noage_preds_aif360 = StandardDataset(preds_df_noage, label_name='credit', protected_attribute_names=['age'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "\n",
    "\n",
    "noage_preds_metrics = BinaryLabelDatasetMetric(noage_preds_aif360, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Mean difference = %f\" % noage_preds_metrics.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % noage_preds_metrics.disparate_impact())\n",
    "\n",
    "orig_vs_noage_preds_metrics = ClassificationMetric(orig_aif360, noage_preds_aif360,\n",
    "                                                   unprivileged_groups=unprivileged_groups,\n",
    "                                                   privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_noage_preds_metrics.error_rate_difference())\n",
    "\n",
    "\n",
    "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_noage_preds_metrics.false_negative_rate(privileged=True))\n",
    "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_noage_preds_metrics.false_negative_rate(privileged=False))\n",
    "print(\"False negative rate ratio = %f\" % orig_vs_noage_preds_metrics.false_negative_rate_ratio())\n",
    "\n",
    "\n",
    "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_noage_preds_metrics.false_positive_rate(privileged=True))\n",
    "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_noage_preds_metrics.false_positive_rate(privileged=False))\n",
    "print(\"False positive rate ratio = %f\" % orig_vs_noage_preds_metrics.false_positive_rate_ratio())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll up -- how do these numbers for our model that doesn't use age compare to the model that *does* use it?\n",
    "\n",
    "Which figures are encoding the same information as age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the data using the reweighting algorithm, then train a classifier to predict credit using the re-weighted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the weights to our training data\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "RW_fit = RW.fit(train_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.71736842, 1.07897059, 0.97097297, 1.19178571, 0.97097297,\n",
       "       0.97097297, 0.71736842, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 0.97097297, 1.07897059, 0.97097297,\n",
       "       1.19178571, 0.97097297, 0.97097297, 1.07897059, 1.19178571,\n",
       "       0.71736842, 0.97097297, 1.19178571, 1.07897059, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 1.19178571,\n",
       "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 1.19178571,\n",
       "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.19178571, 0.97097297, 1.07897059, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.71736842, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.19178571,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
       "       0.97097297, 1.07897059, 0.71736842, 1.07897059, 0.97097297,\n",
       "       0.71736842, 1.07897059, 0.97097297, 1.07897059, 0.71736842,\n",
       "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 1.19178571,\n",
       "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.71736842, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       1.19178571, 1.19178571, 1.07897059, 0.97097297, 1.07897059,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 0.97097297, 1.07897059, 0.71736842,\n",
       "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
       "       1.07897059, 0.71736842, 1.19178571, 1.07897059, 1.07897059,\n",
       "       1.19178571, 1.07897059, 1.07897059, 1.07897059, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 1.07897059,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 1.19178571, 0.97097297,\n",
       "       0.71736842, 1.19178571, 0.71736842, 0.97097297, 1.07897059,\n",
       "       1.07897059, 1.07897059, 0.97097297, 1.07897059, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.71736842,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.71736842, 1.07897059, 0.97097297, 0.97097297,\n",
       "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 1.07897059,\n",
       "       1.07897059, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.71736842, 1.19178571, 0.97097297,\n",
       "       0.97097297, 1.07897059, 1.07897059, 1.19178571, 1.07897059,\n",
       "       0.97097297, 1.07897059, 0.97097297, 1.19178571, 0.97097297,\n",
       "       1.07897059, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.71736842, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 1.07897059,\n",
       "       1.07897059, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.71736842, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.71736842, 0.71736842,\n",
       "       1.07897059, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.07897059,\n",
       "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.71736842, 0.97097297, 1.19178571, 0.97097297,\n",
       "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 1.19178571,\n",
       "       0.71736842, 0.97097297, 1.07897059, 1.07897059, 1.19178571,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.71736842, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.97097297,\n",
       "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 1.07897059, 1.07897059, 0.97097297,\n",
       "       1.07897059, 0.97097297, 1.07897059, 1.19178571, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 1.07897059, 1.19178571,\n",
       "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.19178571, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.71736842,\n",
       "       0.97097297, 0.97097297, 1.19178571, 1.07897059, 0.97097297,\n",
       "       1.07897059, 1.07897059, 0.97097297, 1.07897059, 1.19178571,\n",
       "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.71736842, 1.07897059,\n",
       "       0.71736842, 0.97097297, 0.71736842, 0.97097297, 1.19178571,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.19178571, 1.07897059, 0.97097297, 0.71736842, 0.97097297,\n",
       "       0.97097297, 1.07897059, 1.19178571, 0.97097297, 0.97097297,\n",
       "       1.19178571, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 1.07897059, 0.71736842, 0.71736842,\n",
       "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 0.97097297, 1.19178571, 0.97097297,\n",
       "       1.07897059, 0.71736842, 0.97097297, 0.97097297, 1.07897059,\n",
       "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 1.07897059,\n",
       "       0.71736842, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 1.07897059, 0.97097297, 0.71736842, 0.97097297,\n",
       "       1.07897059, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.97097297, 1.19178571,\n",
       "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.97097297,\n",
       "       0.97097297, 0.97097297, 0.97097297, 1.07897059, 0.71736842,\n",
       "       0.97097297, 0.97097297, 1.19178571, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.19178571, 0.97097297, 0.97097297, 1.07897059,\n",
       "       1.19178571, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.71736842, 0.71736842,\n",
       "       0.97097297, 1.19178571, 1.19178571, 1.19178571, 1.19178571,\n",
       "       1.07897059, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 1.19178571,\n",
       "       1.07897059, 1.19178571, 0.97097297, 1.07897059, 0.97097297,\n",
       "       1.19178571, 0.71736842, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.71736842, 0.97097297, 0.97097297, 0.97097297, 0.97097297,\n",
       "       1.07897059, 0.97097297, 1.19178571, 1.07897059, 0.97097297,\n",
       "       1.19178571, 1.07897059, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 1.07897059, 0.97097297, 1.19178571,\n",
       "       0.97097297, 0.97097297, 0.97097297, 0.71736842, 1.07897059,\n",
       "       1.19178571, 1.07897059, 1.19178571, 0.97097297, 1.07897059,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 1.07897059, 1.07897059, 0.97097297,\n",
       "       1.07897059, 1.07897059, 1.07897059, 0.97097297, 1.07897059,\n",
       "       0.97097297, 0.97097297, 1.07897059, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 0.97097297, 0.97097297, 0.97097297,\n",
       "       0.97097297, 1.07897059, 0.97097297, 1.07897059, 0.97097297])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the actual values of the weights for the training data\n",
    "train_reweighed = RW_fit.transform(train_orig)\n",
    "training_weights = train_reweighed.instance_weights\n",
    "training_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model parameters:  {'penalty': 'l2', 'C': 0.1}\n",
      "Best model AUC:  0.775209340724142\n"
     ]
    }
   ],
   "source": [
    "# Train a model using weights\n",
    "best_lr_weights, _, _, _ = tune_logistic_regression(train_orig_df.drop('age', axis=1), val_orig_df.drop('age', axis=1),\n",
    "                                                                       penalty_types=[\"l1\", \"l2\"], C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000], \n",
    "                                                                       weights=training_weights, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey look-- our AUC fell again. As we've discussed, there's often a tradeoff between fairness and other metrics that stakeholders care about. \n",
    "\n",
    "Let's see if the fairness metrics changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_df_weights = train_orig_df.copy()\n",
    "train_preds_df_weights['credit'] = best_lr_weights.predict(x_train.drop('age', axis=1))\n",
    "train_preds_df_weights['credit'] = train_preds_df_weights.credit.replace({0:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference = -0.079892\n",
      "Disparate Impact = 0.905548\n",
      "\n",
      "Error rate difference (unprivileged error rate - privileged error rate)= 0.067110\n",
      "\n",
      "False negative rate for privledged groups = 0.064865\n",
      "False negative rate for unprivledged groups = 0.089286\n",
      "False negative rate ratio = 1.376488\n",
      "\n",
      "False positive rate for privledged groups = 0.602941\n",
      "False positive rate for unprivledged groups = 0.552632\n",
      "False positive rate ratio = 0.916560\n"
     ]
    }
   ],
   "source": [
    "preds_weights_aif360 = StandardDataset(train_preds_df_weights, label_name='credit', protected_attribute_names=['age'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "preds_weights_metrics = BinaryLabelDatasetMetric(preds_weights_aif360, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Mean difference = %f\" % preds_weights_metrics.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % preds_weights_metrics.disparate_impact())\n",
    "\n",
    "orig_vs_preds_weights_metrics = ClassificationMetric(orig_aif360, preds_weights_aif360,\n",
    "                                                   unprivileged_groups=unprivileged_groups,\n",
    "                                                   privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_preds_weights_metrics.error_rate_difference())\n",
    "\n",
    "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_preds_weights_metrics.false_negative_rate(privileged=True))\n",
    "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_preds_weights_metrics.false_negative_rate(privileged=False))\n",
    "print(\"False negative rate ratio = %f\" % orig_vs_preds_weights_metrics.false_negative_rate_ratio())\n",
    "\n",
    "\n",
    "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_preds_weights_metrics.false_positive_rate(privileged=True))\n",
    "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_preds_weights_metrics.false_positive_rate(privileged=False))\n",
    "print(\"False positive rate ratio = %f\" % orig_vs_preds_weights_metrics.false_positive_rate_ratio())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these numbers compare to the numbers above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Post-process the predictions from the model that we trained using weights by using the calibrated equality of odds algorithm \n",
    "\n",
    "The equality of odds algorithm is a method for adjusting predicted prbabilities to ensure that the false negative rate is equal for the privilged and unprivilged groups. (This also ensures that the true positive rate is equal.) To do so, the algorithm uses the predicted probabilities and determines *two* threshold probabilitties for each group. Above the upper threshold, all members of the group are assigned to the positive class, and below the lower threshold, all members of the group are assigned to the negative class. But between the two thresholds, individuals are randomly assigned a class. \n",
    "\n",
    "For details, see M. Hardt, E. Price, and N. Srebro, “Equality of Opportunity in Supervised Learning,” Conference on Neural Information Processing Systems, 2016.\n",
    "\n",
    "Which definitions of fairness does this post-processing algorithm contradict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our predictions using the aif360 implementation of the equality of odds algorithm\n",
    "eq_odds = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, seed=47)\n",
    "preds_weights_eq_odds_aif360 = eq_odds.fit_predict(orig_aif360, preds_weights_aif360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference = 0.008662\n",
      "Disparate Impact = 5.382979\n",
      "\n",
      "Error rate difference (unprivileged error rate - privileged error rate)= -0.126819\n",
      "\n",
      "False negative rate for privledged groups = 1.000000\n",
      "False negative rate for unprivledged groups = 1.000000\n",
      "False negative rate ratio = 1.000000\n",
      "\n",
      "False positive rate for privledged groups = 0.007353\n",
      "False positive rate for unprivledged groups = 0.026316\n",
      "False positive rate ratio = 3.578947\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "preds_weights_eq_odds_metrics = BinaryLabelDatasetMetric(preds_weights_eq_odds_aif360, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Mean difference = %f\" % preds_weights_eq_odds_metrics.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % preds_weights_eq_odds_metrics.disparate_impact())\n",
    "\n",
    "orig_vs_preds_weights_eq_odds_metrics = ClassificationMetric(orig_aif360, preds_weights_eq_odds_aif360,\n",
    "                                                   unprivileged_groups=unprivileged_groups,\n",
    "                                                   privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"\\nError rate difference (unprivileged error rate - privileged error rate)= %f\" % orig_vs_preds_weights_eq_odds_metrics.error_rate_difference())\n",
    "\n",
    "print(\"\\nFalse negative rate for privledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_negative_rate(privileged=True))\n",
    "print(\"False negative rate for unprivledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_negative_rate(privileged=False))\n",
    "print(\"False negative rate ratio = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_negative_rate_ratio())\n",
    "\n",
    "\n",
    "print(\"\\nFalse positive rate for privledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_positive_rate(privileged=True))\n",
    "print(\"False positive rate for unprivledged groups = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_positive_rate(privileged=False))\n",
    "print(\"False positive rate ratio = %f\" % orig_vs_preds_weights_eq_odds_metrics.false_positive_rate_ratio())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's changed in these metrics? How could the algorithm have caused that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (on training data) before equality of odds algorithm = 0.780000\n",
      "\n",
      "Accuracy (on training data) after equality of odds algorithm = 0.286667\n"
     ]
    }
   ],
   "source": [
    "# Test how accuracy has changed\n",
    "print(\"\\nAccuracy (on training data) before equality of odds algorithm = %f\" % orig_vs_preds_weights_metrics.accuracy())\n",
    "print(\"\\nAccuracy (on training data) after equality of odds algorithm = %f\" % orig_vs_preds_weights_eq_odds_metrics.accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rds_env]",
   "language": "python",
   "name": "conda-env-rds_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
